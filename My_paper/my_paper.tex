% This is a sample document using the University of Minnesota, Morris, Computer Science
% Senior Seminar modification of the ACM sig-alternate style. Much of this content is taken
% directly from the ACM sample document illustrating the use of the sig-alternate class. Certain
% parts that we never use have been removed to simplify the example, and a few additional
% components have been added.

% See https://github.com/UMM-CSci/Senior_seminar_templates for more info and to make
% suggestions and corrections.

\documentclass{sig-alternate}
\usepackage{color}
\usepackage[colorinlistoftodos]{todonotes}

%%%%% Uncomment the following line and comment out the previous one
%%%%% to remove all comments
%%%%% NOTE: comments still occupy a line even if invisible;
%%%%% Don't write them as a separate paragraph
%\newcommand{\mycomment}[1]{}

\begin{document}

% --- Author Metadata here ---
%%% REMEMBER TO CHANGE THE SEMESTER AND YEAR AS NEEDED
\conferenceinfo{UMM CSci Senior Seminar Conference, April 2017}{Morris, MN}

\title{Automating Algorithm Design through Genetic Programming Hyper Heuristics}

\numberofauthors{1}

\author{
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
\alignauthor
Elsa M. Browning\\
	\affaddr{Division of Science and Mathematics}\\
	\affaddr{University of Minnesota, Morris}\\
	\affaddr{Morris, Minnesota, USA 56267}\\
	\email{brow3924@morris.umn.edu}
}

\maketitle
\begin{abstract}
	\todo[inline, color=yellow]{actually write an abstract}
	\todo[inline, color=purple]{Citing stuff to see what it looks like~\cite{lee:2001}~and~\cite{pappa:2014}}
	
\end{abstract}

\keywords{Evolutionary Computation, Genetic Programming, Hyper Heuristics, Autoconstruction}

\section{Introduction}
\label{sec:introduction}
\todo[inline, color=yellow]{Mostly notes right now}

Traditionally, genetic programming solutions to problems evolve, but almost everything else is specified by the system designer \cite{spector:2016}. In autoconstruction, the variation methods are evolving as well.

In sections~\ref{sec:evocomp},~\ref{sec:GP},~and~\ref{sec:HH} we describe necessary background for understanding the rest of the paper. Next, we describe the history of automating algorithm design (AAD) with a focus on hyper-heuristics (HH) optimization in section~\ref{sec:history}. Then, in section~\ref{sec:gpvariants}, we go over a few different genetic programming variants used in HH and the success of two in particular. Finally, we will go over current research being done with stack-based genetic programming (one of the GP variants) in section~\ref{sec:ac}; the stack-based programming language called Push is used in this example along with a technique for AAD called autoconstruction.
\todo[inline, color=yellow]{this is long roadmap. Probably need to shorten}

\section{Background}
\label{sec:background}

\subsection{Evolutionary Computation}
\label{sec:evocomp}

\subsection{Genetic Programming}
\label{sec:GP}

\subsection{Hyper Heuristics}
\label{sec:HH}


\subsection{History of AAD}
\label{sec:history}
Automating algorithm design (AAD) has been investigated by several different areas for about 60 years. In the 1950s, the term ``machine learning" was defined as ``computers programming themselves"~\cite{pappa:2014}. This definition has changed over time to focus more on the learning aspect, but it's still an important area in AAD. While the field of machine learning is out of the scope of our paper, it also plays a major role in the history of AAD and was one of the first fields to do so.

Focusing on the automation of heuristic methods, we can trace the beginnings of hyper heuristics (HH) to the 1960s~\cite{pappa:2014}. Early approaches of HH focused on automatically setting the parameters of evolutionary algorithms~\cite{pappa:2014}. A \textit{parameter} can be thought of as things like population size, crossover, or mutation rates. This is still a question many researchers face when designing HH systems. Generally, parameters are tuned before the evolution starts and controlled during the evolution. There are, however, exceptions.

The idea of \textit{self-adaption}, where an algorithm is able to tune itself to a given problem while solving it, emerged later. Now, we have two major types of HH: \textit{heuristic selection} and \textit{heuristic generation}; the first focuses on selecting the best algorithm from a set of existing heuristics while the latter focuses on generating a new heuristic from the components of existing heuristics~\cite{pappa:2014}. Heuristic generation uses several different methods for the generation process, but we will be focusing on heuristic generation techniques that use genetic programming. 


\section{Genetic Programming Variants}
\label{sec:gpvariants}

\subsection{Tree-based Genetic Programming}
\label{sec:tgp}

\subsection{Stack-based Genetic Programming}
\label{sec:sgp}

\section{Autoconstruction}
\label{sec:ac}
In genetic programming hyper heuristics (GPHH), the goal is to use genetic programming to evolve a program that will solve hard computational search problems. The individual programs serve as potential solutions. In GPHH, only the potential solutions are evolving~--~everything else is specified by the system designer. \todo[inline]{not sure how to phrase previous sentence. Also not sure if I need to clarify what I mean by ``potential solutions"} In autoconstruction, evolution is evolving as well. This happens by encoding the methods of variation into the programs that are evolving so that, as a program evolves, its variation methods also evolve (see section~\ref{sec:push} for how this works).

Prior work on autoconstruction has explored a variety of system designs, but, until recently, none of them have been able to solve hard problems. A new system called Autoconstructive Diversification of Genomes (AutoDoG) has broken this trend by solving at least one hard problem: ``Replace Space with Newline"~\cite{spector:2016}. And, in recent unpublished work, AutoDoG has actually solved a problem that has never been solved before~\cite{eva:2017}.
\todo[inline, color=yellow]{Not sure how much of this is introduction stuff}

\subsection{Push and Plush}
\label{sec:push}

Push is a stack-based programming language with a separate stack for each data type. It was developed for program evolution and autoconstruction was one of the driving forces behind the original design~\cite{spector:2016}. Push programs are strings of instructions, constants, and parentheses with only one syntax requirement: the parentheses must be balanced~\cite{lee:2001}.
\todo[inline]{I am not sure that I should mention prev sentence, because I may want to give examples if this is mentioned...} 
Instructions are executed by putting them on the \texttt{exec} stack. If the necessary arguments are not available for the instruction to be executed, the instruction is skipped. For example, assume all stacks are empty and assume a program says \texttt{(1 2 integer\_add)}. This means push 1 and 2 onto the \texttt{integer} stack and push the \texttt{integer\_add} onto the  take the \texttt{exec} stack~\cite{lee:tutorial}. The \texttt{integer\_add} instruction will try to execute: the first two integers (in this case 1 and 2)  are popped off the top of the \texttt{integer} stack, added together, and the result is pushed back on to the \texttt{integer} stack. If, instead, the program was \texttt{(1 integer\_add)} then it would push 1 onto the \texttt{integer} stack, but skip the \texttt{integer\_add} because there are not enough integers in the \texttt{integer} stack.
\todo[inline]{not sure I should include example inline like this, might want to separate and try to shorten somehow}

\begin{itemize}
	\item Talk about:
	\item Plush
	\item linear genomes
	\item how Plush/linear genomes are beneficial to autoconstruciton
	\item PushGP--a "reasonably standard generational genetic programming system"
	\item PUT PUSHGP IN WITH RESULTS STUFF... BRIEFLY MENTION WHAT IT IS (and that it's not autoconstruciton)
\end{itemize}

\todo[inline, color=yellow]{Turn list into paragraphs}

\subsection{AutoDoG}
\label{sec:autodog}
In this section, we briefly describe some of the key features of AutoDoG.

One thing that the designers of AutoDoG wanted to do was maintain diversity in parent selection. To do this, AutoDoG uses Lexicase Selection, described below: 

In each parent selection event,
\begin{itemize}
	\item start with pool of entire population
	\item filter pool based on how each program performs on individual fitness cases (performed in random order, and one at a time)
	\todo[inline]{HOW do I phrase prev bullet point?}
	\item In each fitness case, only retain programs best on that case.
\end{itemize}
\todo[inline]{List or paragraph?}
\todo[inline]{More/better description of lexi (reread section in helmuth's dissertation)}

Its name comes from the way it filters the population using a kind of ``lexigraphic ordering" of cases \cite{spector:2016}. When tested on a benchmark suite of problems taken from introductory programming textbooks and compared to the results of other current GP parent selection techniques, Lexicase Selection allows for the solution of more problems in fewer generations~\cite{spector:2016}.
\todo[inline]{rephrase prev sentence?}

\todo[inline]{talk about why lee is concerned about cloning (11:20 to 12:22 in video)!!!!}
Most autoconstruction systems have some form of the ``no cloning rule" which prevents an exact copy of a program from moving on to the next generation, and AutoDoG has a form of this as well~\cite{spector:2016}. AutoDoG's version of this requires offspring to pass a more stringent diversification test in order to enter the population~\cite{spector:2016}. This test is described below:
\begin{itemize}
	\item make individual for next generation
	\item individual takes itself as mate and makes temp children.
	\item if temp children differ enough from individual and each other, individual enters next generation's population
	\item else, generate new random individual and repeat previous steps
	\item if random individual fails, generate individual with empty genome and add it to next generation
	\item discard any children after test
\end{itemize}
\todo[inline, color=green]{TO NIC: should I rewrite so that it's not in list form? Paragraph would save space.}

AutoDoG works similarly to PushGP, a reasonably standard generational programming system, but is run with autoconstruction as the sole genetic operator rather than using human designed mutation and crossover operators. AutoDoG's main loop 
\todo[inline, color=purple]{left off 13:00  in video, here (ish) in paper}
``iteratively tests the error of all individuals and then builds the next generation by selecting parents and passing them to genetic operators" \cite{spector:2016}.

\todo[inline, color=yellow]{Describe more/better, summarize long quotes, turn lists into paragraphs}

\subsection{Results}
\label{sec:results}
AutoDoG is unique among autoconstructive systems in that it can solve hard problems. However, as stated by Spector et al.~\cite{spector:2016},
\begin{quotation}
	We do not know which of these, or which combinations of these, may be responsible for the fact that AutoDoG appears to be capable of solving more difficult problems than previous autoconstructive evolution systems.
\end{quotation}
This is because it's hard to separate the pieces; there are a lot of intertwined elements of AutoDoG and Push. Separating them to find exactly which elements contribute to the success without unraveling the entire system is difficult and has not yet been done.
\todo[inline, color=green]{TO NIC: Is this (above) more correct?}

One challenging problem AutoDoG has solved is Replace Space with Newline (RSWN). This is a software synthesis problem which, when given a string, requires the solution to print the string replacing spaces with newlines. It also requires the return of the integer count of the non-whitespace characters. This is complex because it involves multiple data types and outputs, and requires conditionals and iteration or recursion.
AutoDoG does not actually do better than standard PushGP on this problem.
\begin{quotation}
	AutoDoG does not succeed as reliably on this problem as has PushGP in some other configurations, but it does solve the problem approximately 5-10 \% of the time, producing general solutions \cite{spector:2016}
\end{quotation}

``This is not surprising because AutoDoG has to learn how to evolve as well as learn how to solve the problem. That said, recent (unpublished) results look like there might be scenarios where autoconstruction may find solutions to problems we've never solved with regular PushGP, so we may soon have examples where autoconstruction is `better' in some meaningful sense" \todo[inline]{PARAPHRASE QUOTE FROM NIC}

\todo[inline]{More info about new AutoDoG results}

One interesting thing to note is how AutoDoG evolves. In standard PushGP, one can see a steady incline in differences between the genomes of parents and their children. Over time, change occurs at a fairly steady rate and is not very dramatic (from one generation to the next). In AutoDoG, there are huge differences between the genomes of some parents and their children. There was significant change for some individuals from one generation to the next and overall it was a bit erratic.
\todo[inline, color=yellow]{Clean up, more info about weird AutoDoG evolution. Maybe include DL graphs}
\section{Conclusions and Future Work}
\label{sec:conclusion}

\section{Acknowledgments}
\label{sec:acknowledgments}


% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
% my_paper.bib is the name of the BibTex file containing the
% bibliography entries. Note that you *don't* include the .bib ending here.
\bibliography{my_paper}  
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

\todo[inline, color=yellow]{Add more of references to bib, update \cite{eva:2017}, cite Nic?}

\end{document}


\subsection{My nifty subsection}
\label{sec:mysubsection}

I want to refer to Section~\ref{sec:evocomp}
and Figure~\ref{fig:twoColumnFigure}. It would also be nice
to cite~\cite{spector:2016} and~\cite{pappa:2014} and~\cite{eva:2017}.

Let's make an equation:
\[
\textrm{area} = \pi r^2
\]
I want to refer to Section~\ref{sec:typeChangesSpecialChars}
and Figure~\ref{fig:twoColumnFigure}. It would also be nice
to cite~\cite{spector:2016}.
We can also do inline equations: $s = \sum_{i=0}^N x_i$.
I want to refer to Section~\ref{sec:typeChangesSpecialChars}
and Figure~\ref{fig:twoColumnFigure}. It would also be nice
to cite~\cite{spector:2016}.\[
s = \sum_{i=0}^N x_i
\]

Typically, the body of a paper is organized
into a hierarchical structure, with numbered or unnumbered
headings for sections, subsections, sub-subsections, and even
smaller sections.  The command \texttt{\textbackslash section} that
precedes this paragraph is part of such a
hierarchy.\footnote{This is the second footnote.  It
	starts a series of three footnotes that add nothing
	informational, but just give an idea of how footnotes work
	and look. It is a wordy one, just so you see
	how a longish one plays out.} \LaTeX\ handles the numbering
and placement of these headings for you, when you use
the appropriate heading commands around the titles
of the headings.